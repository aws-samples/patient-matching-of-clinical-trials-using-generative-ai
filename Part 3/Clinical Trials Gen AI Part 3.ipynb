{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f52f7c2d-4bac-4616-b78b-58e197810498",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T23:27:13.744975Z",
     "iopub.status.busy": "2024-07-05T23:27:13.744566Z",
     "iopub.status.idle": "2024-07-05T23:27:13.760425Z",
     "shell.execute_reply": "2024-07-05T23:27:13.755390Z",
     "shell.execute_reply.started": "2024-07-05T23:27:13.744942Z"
    }
   },
   "source": [
    "# Clinical Trials Gen AI Workshop Part 3\n",
    "\n",
    "**Note:** This notebook was designed to be run in a JupyterLab space in SageMaker Studio running *Sagemaker Distribution 1.4* on an *ml.t3.medium* instance with 5GB of storage, although other configurations may be supported.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "\"This notebook demonstrates how to apply techniques learned in previous workshop sections to perform processing at scale. This notebook requires a AWS HealthLake Datastore to be created by following the [guide](https://docs.aws.amazon.com/healthlake/latest/devguide/getting-started.html) with **SYNTHEA** as the selected preloaded data type. If you do not have the correct role permissions for AWS Bedrock Batch Inference or an AWS HealthLake Datastore, you can still view the final result HTML page by running all the initial cells through 1.2, and then continuing execution from 1.12 with USE_SAMPLE_BATCH_INFERENCE_OUTPUT = True. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ace6790-5504-4bc7-8e72-538f8f299418",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, import some pre-requisite libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d5fdba-f990-451f-8d5c-ce3e46e5404c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'boto3>=1.35.1'\n",
    "!pip install 'awswrangler>=3.9.1'\n",
    "!pip show boto3\n",
    "!pip show awswrangler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7fe576-8589-48b6-b54b-9a0f7bf48c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from xml.etree import ElementTree\n",
    "import requests\n",
    "from xml.etree import ElementTree\n",
    "import boto3\n",
    "import json\n",
    "import awswrangler as wr\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "import concurrent.futures\n",
    "import time\n",
    "import sagemaker\n",
    "from datetime import date\n",
    "import html\n",
    "from pathlib import Path\n",
    "print(\"All pre-requisite libraries successfully installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8a3402-2fab-4e0b-b5d4-dceb4e8de38f",
   "metadata": {},
   "source": [
    "Setting VERBOSE_LOGGING to ***True*** will generate a lot more cell output but may make it easier to visualize full prompts and can be a useful troubleshooting tool. We use ***False*** by default to make the output easier to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cedf590-76d7-4403-8c1c-1a1fe20b5b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE_LOGGING = False\n",
    "print(f\"Current verbose logging setting is {VERBOSE_LOGGING}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d2fb50-86d2-454d-a2e0-7a0f85c8f955",
   "metadata": {},
   "source": [
    "## 1. Clinical Trial Information Processing\n",
    "\n",
    "### 1.1 Helper function to convert XML to JSON\n",
    "\n",
    "We define a helper function below called parseXmlToJson.\n",
    "\n",
    "It converts an XML like\n",
    "\n",
    "\n",
    "    <eligibility>\n",
    "        <criteria>\n",
    "            <textblock>\n",
    "                TEXT ABOUT ELIGIBILITY CRITERIA\n",
    "            </textblock>\n",
    "        </criteria>\n",
    "    </eligibility>\n",
    "\n",
    "\n",
    "to\n",
    "\n",
    "    {\n",
    "    \t\"eligibility\": {\n",
    "    \t\t\"criteria\": {\n",
    "    \t\t\t\"textblock\": \"TEXT ABOUT ELIGIBILITY CRITERIA\"\n",
    "    \t\t}\n",
    "    \t}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc26a050-f021-4cf7-a194-520ea6026000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseXmlToJson(xml):\n",
    "  response = {}\n",
    "\n",
    "  for child in list(xml):\n",
    "    if len(list(child)) > 0:\n",
    "      response[child.tag] = parseXmlToJson(child)\n",
    "    else:\n",
    "      response[child.tag] = child.text or ''\n",
    "\n",
    "  return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5091e10-5b12-4f5e-8032-b13e41148a92",
   "metadata": {},
   "source": [
    "### 1.2 Download the clinical trial information\n",
    "\n",
    "We identify a clinical trial of interest. \n",
    "\n",
    "Here we've chosen [NCT02697071](https://classic.clinicaltrials.gov/ct2/show/NCT02697071) titled ***Ketamine for Acute Migraine in the Emergency Department***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00517bd-923e-437c-9b7a-398c00e1c4cd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# store the clinicaltrials.gov id in the variable\n",
    "clinical_trial_id = 'NCT02697071'\n",
    "\n",
    "# We download the clinical trial information\n",
    "# from clinicaltrials.gov in xml format.\n",
    "query_string = f\"https://clinicaltrials.gov/ct2/show/{clinical_trial_id}?displayxml=true\"\n",
    "response = requests.get(query_string)\n",
    "\n",
    "# Parse the XML to JSON object using the helper function\n",
    "# we defined above\n",
    "events = ElementTree.fromstring(response.content)\n",
    "jsonObj = parseXmlToJson(events)\n",
    "\n",
    "# The clinical trials website/information download includes \n",
    "# a lot of information about the clinical trial, we are only \n",
    "# interested in the criteria for the study\n",
    "\n",
    "criteria = jsonObj[\"eligibility\"][\"criteria\"][\"textblock\"]\n",
    "print(criteria)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa8daeb-845e-4dea-a3c0-616e84bd634f",
   "metadata": {},
   "source": [
    "### 1.3 Helper function for invoking an LLM using Amazon Bedrock\n",
    "\n",
    "Here we define a helper function that leverages Amazon Bedrock and provides an API to interact with Large Language Models (LLM). Here we have chosen to leverage Claude 3 Sonnet. We also provide an estimate of the number of input tokens which measures the length of the input prompt to the LLM. This estimate can be found by dividing the number of characters in the input prompt by 6. Claude 3 Sonnet supports a 200,000 input token context limit which roughly translates to 500 pages, or about the size of a book.\n",
    "\n",
    "Claude uses a system role for enhanced accuracy and improved focus. Learn more details at https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/system-prompts.\n",
    "\n",
    "We set the inference parameter temperature to 0 which reduces randomness (creativity) in responses so the model is more likely to provide the same output when invoked with the same input. You can find additional information about inference parameters at https://docs.aws.amazon.com/bedrock/latest/userguide/inference-parameters.html.\n",
    "\n",
    "You must add model access for the Claude 3 Sonnet model in the current region as part of this workship via the Bedrock console if you have not done so previously. Follow the steps at https://docs.aws.amazon.com/bedrock/latest/userguide/model-access-modify.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007a8a02-4a11-4e9c-9953-c9693c9429c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEDROCK_CLIENT = boto3.client(service_name='bedrock-runtime')\n",
    "CLAUDE_3_SONNET_MODEL_ID = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "\n",
    "def invoke_claude_v3(input_prompt, system_message=\"You are acting as a researcher evaluating if a patient profile is suitable for a clinical trial.\"):\n",
    "    modelId = CLAUDE_3_SONNET_MODEL_ID\n",
    "    \n",
    "    body=json.dumps(\n",
    "        {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": 2000,\n",
    "            \"temperature\": 0, # see https://docs.aws.amazon.com/bedrock/latest/userguide/inference-parameters.html for more information\n",
    "            \"system\": system_message,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": input_prompt}]\n",
    "        }  \n",
    "    )  \n",
    "\n",
    "    concat_index = 1500\n",
    "    if VERBOSE_LOGGING or len(body) <= concat_index:\n",
    "        print(f\"Invoking {modelId} with input: {body}\")\n",
    "    else:\n",
    "        index_to_concat = min(concat_index, len(body))\n",
    "        print(f\"Invoking {modelId} with concatenated input (set VERBOSE_LOGGING to True for full input): {body[0:index_to_concat]}\\n\\n...\\n\\n{body[-1*index_to_concat:]}\")\n",
    "\n",
    "    input_token_estimate = int(len(input_prompt)/6)\n",
    "    print(f\"\\n\\n Estimated input tokens: {input_token_estimate}\")\n",
    "\n",
    "    print(\"\\n\\n Response below (Please wait for the response output before proceeding to the next cell as the request may take time to process):\\n\")\n",
    "\n",
    "    response = BEDROCK_CLIENT.invoke_model(body=body, modelId=modelId)\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "\n",
    "    response = response_body['content'][0]['text']\n",
    "\n",
    "    print(response)\n",
    "   \n",
    "    return response\n",
    "\n",
    "# Validate bedrock model access\n",
    "try:\n",
    "    invoke_claude_v3(\"ping, respond with \\\"pong\\\"\", \"\")\n",
    "    print(\"\\n\")\n",
    "    print(\"Bedrock model access confirmed\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"\\n\")\n",
    "    print(\"Bedrock model access failed. Please ensure you have the correct permissions and the model is available in your region. Follow the documentation at https://docs.aws.amazon.com/bedrock/latest/userguide/model-access-modify.html to add model access.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d305237-0bf2-4b49-b569-3250492dd074",
   "metadata": {},
   "source": [
    "### 1.4 Parsing Clinical Trail Eligibility\n",
    "\n",
    "The eligibility criteria that we have when downloading from the clinicaltrials.gov website comes as a text block. This can contain new line characters, bullet points, etc. We first want to transform this into a structured format, like a JSON object, where we can iterate through each study criterion using code. We could potentially write a rules-based engine to perform this work, but there are hundreds of variations in how researchers describe study criteria. Instead, we will leverage an LLM Assistant to perform this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cee8e4-ed53-4ea8-b729-a3f3d391c1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = f\"\"\"\n",
    "\n",
    "We are looking to process the inclusion and exclusion criteria for a scientific study as a json object. \n",
    "Inside <study></study> XML tags is the inclusion and exclusion criteria for a scientific study. \n",
    "Put your answerser to the user inside <answer></answer> XML tags\n",
    "\n",
    "<study>{criteria}</study>\n",
    "\"\"\"\n",
    "\n",
    "input_prompt += \"\"\"\n",
    "\n",
    "Here is an example:\n",
    "<example>\n",
    "{\n",
    "\n",
    "\"Inclusion Criteria\":[\n",
    "\"Inclusion Criteria 1\",\n",
    "\"Inclusion Criteria 2\"],\n",
    "\"Exclusion Criteria\":[\n",
    "\"Exclusion Criteria 1\",\n",
    "\"Exclusion Criteria 2\"],\n",
    "\n",
    "}\n",
    "</example>\n",
    "\"\"\"\n",
    "\n",
    "llm_study_response = invoke_claude_v3(input_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ab05f2-6f88-44be-93d9-9eba7094be92",
   "metadata": {},
   "source": [
    "### 1.5 Parsing Clinical Trial Eligibility Response from LLM to Python object\n",
    "\n",
    "We will now process the output of the LLM assistant and make it possible to interact with the structured output as a python json object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f55b8a-22d4-44f8-b734-e14127f3ac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_claude_output = llm_study_response.replace(\"\\n\",\"\")\n",
    "xml_start_loc = raw_claude_output.find(\"<answer>\") + 8\n",
    "xml_end_loc = raw_claude_output.find(\"</answer>\")\n",
    "\n",
    "if xml_start_loc == -1 or xml_end_loc == -1:\n",
    "    raise Exception(\"No <answer> xml tag detected in claude output\")\n",
    "\n",
    "study_criteria_json = json.loads(raw_claude_output.replace(\"\\n\",\"\")[xml_start_loc:xml_end_loc])\n",
    "\n",
    "study_criteria = json.dumps(study_criteria_json, indent=4)\n",
    "print(study_criteria)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7282680-9479-4f37-abd7-022cc62e410e",
   "metadata": {},
   "source": [
    "## 2. Batch Clinical Trial Eligibility Processing\n",
    "\n",
    "When processing large amounts of data using Amazon Bedrock, we will use the [batch inference](https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference.html) feature. This specific feature offers 50% off the on-demand inference price. Use batch inference to run multiple inference requests asynchronously, and improve the performance of model inference on large datasets. Completion time of batch inference depends on various factors like the size of the job, but you can expect completion timeframe of a typical job within 24 hours. Learn more by reviewing the [launch annoucement](https://aws.amazon.com/about-aws/whats-new/2024/08/amazon-bedrock-fms-batch-inference-50-price/) of this feature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17548306-0ea6-4f23-9a71-3e79292fa60b",
   "metadata": {},
   "source": [
    "### 2.1 Configuration Setup\n",
    "\n",
    "We intialize all of our AWS SDK clients here. We also retreive the S3 bucket that is created by default in this Sagemaker domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba1dbfc-3351-4bd0-bcc8-3f420a66b639",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEDROCK_CLIENT = boto3.client(service_name='bedrock')\n",
    "S3_CLIENT = boto3.client('s3')\n",
    "SAGEMAKER_SESSION = sagemaker.Session()\n",
    "SAGEMAKER_S3_BUCKET = SAGEMAKER_SESSION.default_bucket()\n",
    "print(f\"Sagemaker S3 Bucket is {SAGEMAKER_S3_BUCKET}\")\n",
    "STS_CLIENT = boto3.client(\"sts\")\n",
    "ACCOUNT_ID = STS_CLIENT.get_caller_identity()[\"Account\"]\n",
    "print(f\"Current ACCOUNT_ID is {ACCOUNT_ID}\")\n",
    "part_3_resource_folder = \"[Resources] Clinical Trials Gen AI Part 3\"\n",
    "print(f\"part_3_resource_folder is {part_3_resource_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb446f71-2135-48b6-8ab4-7b215372b01f",
   "metadata": {},
   "source": [
    "### 2.2 Define Helper Functions for FHIR Retreival\n",
    "\n",
    "We define helper functions that extract key data fields from patient, encounter, observation, condition, procedure, and medication FHIR resources for each patient's record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea82e993-c846-4919-8500-feeda1d3757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseFHIRObj(text, search):\n",
    "    if VERBOSE_LOGGING:\n",
    "        print(f'text: {text}')\n",
    "        print(f'search: {search}')\n",
    "    if not isinstance(text, str) or len(text) < len(search) or text == 'Not Recorded':\n",
    "        if VERBOSE_LOGGING:\n",
    "            print('Text not found')\n",
    "        return \"Not Recorded\"\n",
    "    \n",
    "    index = text.find(f'{search}=')\n",
    "    found_text = text[index + len(search) + 1 : text.find(',', index)]\n",
    "\n",
    "    if VERBOSE_LOGGING:\n",
    "        print(f'found_text: {found_text}')\n",
    "    \n",
    "    return found_text\n",
    "\n",
    "def compact_patient_resource(df):  \n",
    "    compact_patient = {\n",
    "        \"gender\" : df[\"gender\"].iloc[0],\n",
    "        \"birthdate\" : df[\"birthdate\"].iloc[0]\n",
    "    }\n",
    "    return compact_patient\n",
    "\n",
    "def compact_encounters_resource(df):\n",
    "    scoped_df = df[[\"type\",\"period\",\"reasoncode\"]].fillna('Not Recorded')\n",
    "\n",
    "    new_encounter_list = []\n",
    "    \n",
    "    for index, row in scoped_df.iterrows():\n",
    "        encounter_type = parseFHIRObj(row['type'], 'text')\n",
    "\n",
    "        encounter_start = parseFHIRObj(row['period'], 'start')\n",
    "\n",
    "        encounter_end = parseFHIRObj(row['period'], 'end')\n",
    "\n",
    "        encounter_reasoncode = parseFHIRObj(row['reasoncode'], 'display')\n",
    "\n",
    "        encounter = {\n",
    "            'encounter_type': encounter_type,\n",
    "            'encounter_start': encounter_start,\n",
    "            'encounter_end': encounter_end,\n",
    "            'encounter_reasoncode': encounter_reasoncode\n",
    "        }\n",
    "\n",
    "        new_encounter_list.append(encounter)\n",
    "\n",
    "    if len(new_encounter_list) == 0:\n",
    "        return \"None\"\n",
    "    else:\n",
    "        return new_encounter_list\n",
    "    \n",
    "def compact_observations_resource(df):\n",
    "    scoped_df = df[[\"category\",\"code\",\"effectivedatetime\",\"valuequantity\", \"component\"]].fillna('Not Recorded')\n",
    "    \n",
    "    new_observations_list = []\n",
    "    \n",
    "    for index, row in scoped_df.iterrows():\n",
    "        observation_category = parseFHIRObj(row['category'], 'code')\n",
    "\n",
    "        observation_code = parseFHIRObj(row['code'], 'text')\n",
    "\n",
    "        observation_time = row['effectivedatetime']\n",
    "\n",
    "        if row['component'] == 'Not Recorded':\n",
    "            observation_value = parseFHIRObj(row['valuequantity'], 'value')\n",
    "        else:\n",
    "            observation_value = row['component']\n",
    "\n",
    "        observation_unit = parseFHIRObj(row['valuequantity'], 'unit')\n",
    "    \n",
    "\n",
    "        observation = {\n",
    "            'observation_category': observation_category,\n",
    "            'observation_code': observation_code,\n",
    "            'observation_time': observation_time,\n",
    "            'observation_value': observation_value,\n",
    "            'observation_unit': observation_unit,\n",
    "        }\n",
    "\n",
    "        new_observations_list.append(observation)\n",
    "\n",
    "    if len(new_observations_list) == 0:\n",
    "        return \"None\"\n",
    "    else:\n",
    "        return new_observations_list\n",
    "\n",
    "def compact_conditions_resource(df):\n",
    "    scoped_df = df[[\"code\",\"onsetdatetime\",\"abatementdatetime\",\"recordeddate\"]].fillna('Not Recorded')\n",
    "    \n",
    "    new_conditions_list = []\n",
    "    \n",
    "    for index, row in scoped_df.iterrows():\n",
    "        condition_code = parseFHIRObj(row['code'], 'text')\n",
    "\n",
    "        condition_onsetdatetime = row['onsetdatetime']\n",
    "\n",
    "        condition_abatementdatetime = row['abatementdatetime']\n",
    "\n",
    "        condition_recordeddate = row['recordeddate']\n",
    "\n",
    "        condition = {\n",
    "            'condition_code': condition_code,\n",
    "            'condition_onsetdatetime': condition_onsetdatetime,\n",
    "            'condition_abatementdatetime': condition_abatementdatetime,\n",
    "            'condition_recordeddate': condition_recordeddate,\n",
    "        }\n",
    "\n",
    "        new_conditions_list.append(condition)\n",
    "\n",
    "    if len(new_conditions_list) == 0:\n",
    "        return \"None\"\n",
    "    else:\n",
    "        return new_conditions_list\n",
    "\n",
    "def compact_procedures_resource(df):\n",
    "    scoped_df = df[[\"code\",\"performedperiod\",\"reasonreference\"]].fillna('Not Recorded')\n",
    "    \n",
    "    new_procedures_list = []\n",
    "    \n",
    "    for index, row in scoped_df.iterrows():\n",
    "        procedure_code = parseFHIRObj(row['code'], 'text')\n",
    "\n",
    "        procedure_start = parseFHIRObj(row['performedperiod'], 'start')\n",
    "        \n",
    "        procedure_end = parseFHIRObj(row['performedperiod'], 'end')\n",
    "\n",
    "        procedure_reason = parseFHIRObj(row['reasonreference'], 'display')\n",
    "\n",
    "        procedure = {\n",
    "            'procedure_code': procedure_code,\n",
    "            'procedure_start': procedure_start,\n",
    "            'procedure_end': procedure_end,\n",
    "            'procedure_reason': procedure_reason,\n",
    "        }\n",
    "\n",
    "        new_procedures_list.append(procedure)\n",
    "    \n",
    "    if len(new_procedures_list) == 0:\n",
    "        return \"None\"\n",
    "    else:\n",
    "        return new_procedures_list\n",
    "\n",
    "def compact_medications_resource(df):\n",
    "    scoped_df = df[[\"medicationcodeableconcept\",\"authoredon\"]].fillna('Not Recorded')\n",
    "    \n",
    "    new_medications_list = []\n",
    "    \n",
    "    for index, row in scoped_df.iterrows():\n",
    "        medication_name = parseFHIRObj(row['medicationcodeableconcept'], 'text')\n",
    "\n",
    "        medication_prescribed_time = row['authoredon']\n",
    "\n",
    "        medication = {\n",
    "            'medication_name': medication_name,\n",
    "            'medication_prescribed_time': medication_prescribed_time,\n",
    "        }\n",
    "\n",
    "        new_medications_list.append(medication)\n",
    "\n",
    "    if len(new_medications_list) == 0:\n",
    "        return \"None\"\n",
    "    else:\n",
    "        return new_medications_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcd8008-ce55-4957-8023-e97cdda0a2f0",
   "metadata": {},
   "source": [
    "### 2.3 Define Helper Functions for Batch Processing\n",
    "\n",
    "Here, we define a helper function to create a random ID that can be used for AWS Bedrock Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f41c3b-eba3-43fb-9645-cd62c10cc3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_record_id():\n",
    "    return ''.join(random.choices(string.ascii_uppercase + string.digits, k=12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd448d32-4d9d-4695-a785-e73ff6483ac4",
   "metadata": {},
   "source": [
    "### 2.4 Find AWS Healthlake Athena Database\n",
    "\n",
    "This jupyter notebook assumes you have created an AWS Healthlake Datastore by following the [documentation](https://docs.aws.amazon.com/healthlake/latest/devguide/create-data-store.html), ensure that \"Preload sample data\" checkbox is selected. This will automatically setup Athena access if the IAM role you're using contains the \"AWSLakeFormationDataAdmin\" policy. If your IAM role does not contain the necessary permissions, ask your AWS IAM administrator to setup the Athena integration as described in the [documentation](https://docs.aws.amazon.com/healthlake/latest/devguide/search-healthlake.html). \n",
    "\n",
    "Here, we list all of the athena databases that end with \"_healthlake_view\". By default, we select the first database in the list but you can tune this selection by changing the value of DATABASE_SELECTION_INDEX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde20f18-2dfa-438b-b226-2c6f8bac1f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_filtered_athena_databases(catalog_name):\n",
    "    # Initialize the Athena client\n",
    "    athena_client = boto3.client('athena')\n",
    "\n",
    "    # Create a paginator for the list_databases operation\n",
    "    paginator = athena_client.get_paginator('list_databases')\n",
    "\n",
    "    # Initialize an empty list to store database names\n",
    "    database_names = []\n",
    "\n",
    "    # Paginate through the results\n",
    "    for page in paginator.paginate(CatalogName=catalog_name):\n",
    "        for database in page['DatabaseList']:\n",
    "            db_name = database['Name']\n",
    "            # Filter databases that end with \"_healthlake_view\"\n",
    "            if db_name.endswith(\"_healthlake_view\"):\n",
    "                database_names.append(db_name)\n",
    "\n",
    "    return database_names\n",
    "\n",
    "ATHENA_CATALOG_NAME = 'AwsDataCatalog'\n",
    "filtered_databases = list_filtered_athena_databases(ATHENA_CATALOG_NAME)\n",
    "print(f\"Found {len(filtered_databases)} matching databases:\")\n",
    "print(filtered_databases)\n",
    "DATABASE_SELECTION_INDEX = 0\n",
    "print(f\"Selected DATABASE_SELECTION_INDEX: {DATABASE_SELECTION_INDEX}\")\n",
    "HEALTHLAKE_ATHENA_DATABASE = filtered_databases[DATABASE_SELECTION_INDEX]\n",
    "print(f\"Using HEALTHLAKE_ATHENA_DATABASE: {HEALTHLAKE_ATHENA_DATABASE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cda26ac-f403-4763-9d6e-2de8b48a886d",
   "metadata": {},
   "source": [
    "### 2.5 Define Helper Function for Patient Information Retreival from AWS Healthlake\n",
    "\n",
    "This helper function uses the [AWS SDK for pandas (awswrangler)](https://aws-sdk-pandas.readthedocs.io/en/stable/) to retreive patient data from AWS Healthlake using SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea78bd7-b48a-430b-85d8-e658820b1371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_input_prompt_patient(input_prompt, patient_id, study_criteria):\n",
    "    if VERBOSE_LOGGING:\n",
    "        print(f\"Starting Athena queries to retreive patient data from Healthlake for {patient_id}:\")\n",
    "\n",
    "    patient_query = f\"select * from patient where id = '{patient_id}'\"\n",
    "    df = wr.athena.read_sql_query(patient_query, workgroup=\"primary\", database=HEALTHLAKE_ATHENA_DATABASE, ctas_approach=False, athena_cache_settings={\"max_cache_seconds\": 2628000, \"max_cache_query_inspections\": 500})\n",
    "    sample_patient_resource = compact_patient_resource(df)\n",
    "\n",
    "    encounters_query = f\"select * from encounter where subject.reference = 'Patient/{patient_id}'\"\n",
    "    df = wr.athena.read_sql_query(encounters_query, workgroup=\"primary\", database=HEALTHLAKE_ATHENA_DATABASE, ctas_approach=False, athena_cache_settings={\"max_cache_seconds\": 2628000, \"max_cache_query_inspections\": 500})\n",
    "    sample_patient_encounters = compact_encounters_resource(df)\n",
    "\n",
    "    observations_query = f\"select * from observation where subject.reference = 'Patient/{patient_id}'\"\n",
    "    df = wr.athena.read_sql_query(observations_query, workgroup=\"primary\", database=HEALTHLAKE_ATHENA_DATABASE, ctas_approach=False, athena_cache_settings={\"max_cache_seconds\": 2628000, \"max_cache_query_inspections\": 500})\n",
    "    sample_patient_observations = compact_observations_resource(df)\n",
    "\n",
    "    conditions_query = f\"select * from condition where subject.reference = 'Patient/{patient_id}'\"\n",
    "    df = wr.athena.read_sql_query(conditions_query, workgroup=\"primary\", database=HEALTHLAKE_ATHENA_DATABASE, ctas_approach=False, athena_cache_settings={\"max_cache_seconds\": 2628000, \"max_cache_query_inspections\": 500})\n",
    "    sample_patient_conditions = compact_conditions_resource(df)\n",
    "\n",
    "    procedure_query = f\"select * from procedure where subject.reference = 'Patient/{patient_id}'\"\n",
    "    df = wr.athena.read_sql_query(procedure_query, workgroup=\"primary\", database=HEALTHLAKE_ATHENA_DATABASE, ctas_approach=False, athena_cache_settings={\"max_cache_seconds\": 2628000, \"max_cache_query_inspections\": 500})\n",
    "    sample_patient_procedures = compact_procedures_resource(df)\n",
    "\n",
    "    medication_request_query = f\"select * from medicationrequest where subject.reference = 'Patient/{patient_id}'\"\n",
    "    df = wr.athena.read_sql_query(medication_request_query, workgroup=\"primary\", database=HEALTHLAKE_ATHENA_DATABASE, ctas_approach=False, athena_cache_settings={\"max_cache_seconds\": 2628000, \"max_cache_query_inspections\": 500})\n",
    "    sample_patient_medication_requests = compact_medications_resource(df)\n",
    "    \n",
    "    input_prompt = input_prompt.replace(\"<patient>REPLACE</patient>\",f\"<patient>{sample_patient_resource}</patient>\")\n",
    "    input_prompt = input_prompt.replace(\"<encounter>REPLACE</encounter>\",f\"<encounter>{sample_patient_encounters}</encounter>\")\n",
    "    input_prompt = input_prompt.replace(\"<condition>REPLACE</condition>\",f\"<condition>{sample_patient_conditions}</condition>\")\n",
    "    input_prompt = input_prompt.replace(\"<observation>REPLACE</observation>\",f\"<observation>{sample_patient_observations}</observation>\")\n",
    "    input_prompt = input_prompt.replace(\"<procedure>REPLACE</procedure>\",f\"<procedure>{sample_patient_procedures}</procedure>\")\n",
    "    input_prompt = input_prompt.replace(\"<medicationRequest>REPLACE</medicationRequest>\",f\"<medicationRequest>{sample_patient_medication_requests}</medicationRequest>\")\n",
    "    input_prompt = input_prompt.replace(\"<study>REPLACE</study>\",f\"<study>{study_criteria}</study>\")\n",
    "\n",
    "    bedrock_body= {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 2000,\n",
    "        \"temperature\": 0,\n",
    "        \"system\": \"You are acting as a researcher evaluating if a patient profile is suitable for a clinical trial.\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": input_prompt}]\n",
    "    }   \n",
    "\n",
    "    consolidated_patient_object = {\n",
    "        \"FHIR Patient ID\": patient_id,\n",
    "        \"patient\": sample_patient_resource,\n",
    "        \"encounters\": sample_patient_encounters,\n",
    "        \"conditions\": sample_patient_conditions,\n",
    "        \"observations\": sample_patient_observations,\n",
    "        \"procedures\": sample_patient_procedures,\n",
    "        \"medication requests\": sample_patient_medication_requests,\n",
    "    }\n",
    "\n",
    "    if VERBOSE_LOGGING:\n",
    "        print(f\"Finished retreiving patient data from Healthlake for {patient_id}:\")\n",
    "    \n",
    "    return (patient_id, generate_random_record_id(), bedrock_body, consolidated_patient_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af182e8-e19a-434b-8ee7-a967c774c6fb",
   "metadata": {},
   "source": [
    "### 2.6 Retreive Patient ID List\n",
    "\n",
    "We select a list of patient IDs from a file or retreive all patients in our AWS Healthlake database that we are interested in examining for clinical trial eligbility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00819049-b32a-4f67-a4f7-9a8c0ade9931",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_FULL_HEALTHLAKE_DATABASE = True # set this to False to use a patient id list in txt format\n",
    "\n",
    "if not USE_FULL_HEALTHLAKE_DATABASE:\n",
    "    def read_list_of_patient_ids(filename):\n",
    "        file = open(filename, 'r')\n",
    "        lines = file.readlines()\n",
    "        return list(map(lambda s: s.strip(), lines))\n",
    "\n",
    "    patient_id_list = read_list_of_patient_ids(f\"{part_3_resource_folder}/patient_id_list.txt\")\n",
    "\n",
    "else:\n",
    "    patient_list_query = \"select id from patient\"\n",
    "    df = wr.athena.read_sql_query(patient_list_query, workgroup=\"primary\", database=HEALTHLAKE_ATHENA_DATABASE, ctas_approach=False, athena_cache_settings={\"max_cache_seconds\": 2628000, \"max_cache_query_inspections\": 500})\n",
    "    patient_id_list = df['id'].tolist()\n",
    "\n",
    "print(patient_id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a239fc-0fc9-48fc-8557-eea26936eae1",
   "metadata": {},
   "source": [
    "### 2.7 Multi-threaded Prompt Retreival for Patients\n",
    "\n",
    "We now define the input prompt to our LLM and we use the python multi-threaded concurrent library to retrieve  patient data in parallel. This takes around 2 minutes on the recommended *t3.medium* instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83caad4d-3a6e-4eaf-bf70-d63ae66dbf80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_prompt = \"\"\"\n",
    "\n",
    "We are looking to evaluate if a patient is a good candidate based on the criteria for a scientific study.\n",
    "\n",
    "Inside <patient></patient> XML tags is the patient profile. \n",
    "Inside <encounter></encounter> XML tags is a list of the patient's encounters with healthcare providers.\n",
    "Inside <observation></observation> XML tags is a list of the patient's observations from healthcare providers.\n",
    "Inside <condition></condition> XML tags is a list of the patient's conditions diagnosed by healthcare providers.\n",
    "Inside <procedure></procedure> XML tags is a list of the patient's procedures from healthcare providers.\n",
    "Inside <medicationRequest></medicationRequest> XML tags is a list of the patient's prescription medication requests.\n",
    "Inside <study></study> XML tags is the criteria of a scientific study\n",
    "\n",
    "<patient>REPLACE</patient>\n",
    "\n",
    "<encounter>REPLACE</encounter>\n",
    "\n",
    "<observation>REPLACE</observation>\n",
    "\n",
    "<condition>REPLACE</condition>\n",
    "\n",
    "<procedure>REPLACE</procedure>\n",
    "\n",
    "<medicationRequest>REPLACE</medicationRequest>\n",
    "\n",
    "<study>REPLACE</study>\n",
    "\n",
    "Please provide your response for each of the study criteria as\n",
    "\n",
    "(good candidate) Yes this patient is a good candidate for the scientific study.\n",
    "(likely good candidate) It is likely that this patient is a good candidate for the scientific study but more information might be needed.\n",
    "(likely bad candidate) It is unlikely that this patient is a good candidate for the scientific study but more information might be needed.\n",
    "(bad candidate) No this patient is not a good candidate for the scientific study.\n",
    "\n",
    "and an explanation with references to the patient information for why you chose each response\n",
    "\n",
    "in a JSON reponse.\n",
    "\n",
    "If there is no evidence in the patient record for a exclusion condition, choose good candidate.\n",
    "\n",
    "If the Inclusion criteria you are evaluating is an age range, choose good candidate if the patient falls within the age range, choose bad candidate if the patient falls outside the age range.\n",
    "\n",
    "\"\"\" + f\"Today's date is {date.today()}\" + \"\"\"\n",
    "\n",
    "Put your answers to the user inside <answer></answer> XML tags\n",
    "\n",
    "Here is an example:\n",
    "<example>\n",
    "{\n",
    "    \"Inclusion Criteria\":{\n",
    "        \"Age between 12-55\": {\n",
    "            \"Answer\":\"good candidate\",\n",
    "            \"Explanation\":\"The person's birthdate was 1996-12-10 which makes the patient 28 years old\"\n",
    "        },\n",
    "        \"History of Disease A\":{\n",
    "            \"Answer\":\"likely good candidate\",\n",
    "            \"Explanation\":\"There is evidence that the person has had Disease A in the encounter on 2022-01-31\"\n",
    "        }\n",
    "    },\n",
    "    \"Exclusion Criteria\":{\n",
    "        \"Age greater than 55\": {\n",
    "            \"Answer\":\"good candidate\",\n",
    "            \"Explanation\":\"The person's birthdate was 1996-12-10 which makes the patient 28 years old which is not an age greater than 55\"\n",
    "        },\n",
    "        \"History of Disease B\": {\n",
    "            \"Answer\":\"likely bad candidate\",\n",
    "            \"Explanation\":\"I did not find evidence of the patient for Disease B in the previous encounters but they had a related disease of Disease C as referenced by the doct's visit on 2021-10-13\"\n",
    "        },\n",
    "    }\n",
    "}\n",
    "</example>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(\"Starting parallel retreival of patient data from Healthlake. Please wait for completion before moving to the next cell.\")\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers = 20) as executor:\n",
    "    # Submit tasks to the executor\n",
    "    futures = [executor.submit(get_llm_input_prompt_patient, input_prompt, patient_id, study_criteria) for patient_id in patient_id_list]\n",
    "    # Collect the results\n",
    "    results = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "\n",
    "print(\"Finished Retreiveing all patient data from Healthlake.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fb7f07-041b-4937-bf1c-092566ff5e61",
   "metadata": {},
   "source": [
    "### 2.8 Generate Bedrock Batch Inference Input File\n",
    "\n",
    "For each patient we examine, we store the patient information as individual files in S3 so we can view them later. For each patient, we store the full text of the prompt we want the LLM to evaluate as a new line in json format. One of the requirements for Bedrock Batch Inference is a minimum of 1000 records per job. Since our demo doesn't have this many records, we will have to add some additional blank records to meet the minimum required per job. Additional quotas and requirements can be found in the [documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/quotas.html#quotas-batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7497ff97-ec0f-4b8a-9842-895d4a835d6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dummy_record():\n",
    "    return {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 1,\n",
    "        \"temperature\": 0,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"I\"}]\n",
    "    }\n",
    "\n",
    "record_id_to_patient_info_map = {}\n",
    "\n",
    "batch_job_dict = []\n",
    "\n",
    "for result in results:\n",
    "    patient_id = result[0]\n",
    "    \n",
    "    consolidated_patient_info = json.dumps(result[3], indent=4)\n",
    "\n",
    "    record_id = result[1]\n",
    "\n",
    "    record_id_to_patient_info_map[record_id] = consolidated_patient_info\n",
    "\n",
    "    llm_input = result[2]\n",
    "    \n",
    "    patient = {\n",
    "        \"recordId\" : record_id,\n",
    "        \"modelInput\" : llm_input\n",
    "    }\n",
    "    batch_job_dict.append(patient)\n",
    "\n",
    "# Add dummy records to meet the minimum required per job\n",
    "while len(batch_job_dict) < 1000:\n",
    "    batch_job_dict.append({\n",
    "        \"recordId\" : generate_random_record_id(),\n",
    "        \"modelInput\" : dummy_record()\n",
    "    })\n",
    "\n",
    "print(f\"batch_job_dict length: {len(batch_job_dict)}\")\n",
    "\n",
    "batch_inference_input_filename = \"batch_inference_input.jsonl\"\n",
    "with open(batch_inference_input_filename, \"w\") as outfile:\n",
    "    for job in batch_job_dict:\n",
    "        outfile.write(json.dumps(job) + \"\\n\")\n",
    "\n",
    "print(f\"Generated batch inference input as {batch_inference_input_filename}\")\n",
    "\n",
    "record_id_to_patient_info_map_filename = \"record_id_to_patient_info_map.json\"\n",
    "with open(record_id_to_patient_info_map_filename, \"w\") as outfile:\n",
    "    outfile.write(json.dumps(record_id_to_patient_info_map))\n",
    "\n",
    "print(f\"Generated record id to patient info map as {record_id_to_patient_info_map_filename}\")\n",
    "\n",
    "if VERBOSE_LOGGING:\n",
    "    print(f\"record_id_to_patient_info_map: {json.dumps(record_id_to_patient_info_map)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e2a534-d6db-43ab-94a6-a2a16bf342b0",
   "metadata": {},
   "source": [
    "### 2.9 Upload Bedrock Batch Inference Input File\n",
    "\n",
    "Batch inference requires the input file to be stored in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda7a1c0-2f5f-4dcb-8143-0e9e46f4b708",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_batch_processing_input_folder = \"Clinical_Trials_Gen_AI_Workshop_Batch_Inference_Input\"\n",
    "bedrock_batch_processing_s3_input_filepath = f\"{bedrock_batch_processing_input_folder}/{batch_inference_input_filename}\"\n",
    "S3_CLIENT.upload_file(batch_inference_input_filename, SAGEMAKER_S3_BUCKET, bedrock_batch_processing_s3_input_filepath)\n",
    "print(f\"Successfully uploaded {batch_inference_input_filename} to s3://{SAGEMAKER_S3_BUCKET}/{bedrock_batch_processing_s3_input_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd174c7-e077-46e2-81b8-3f10d58bbf57",
   "metadata": {},
   "source": [
    "### 2.10 Bedrock Batch Inference Role Configuration\n",
    "\n",
    "Bedrock Batch Inference requires a role that has permissions to read the S3 Input file and write the output as a file in the designated location in S3. By default, we will use the name **\"Bedrock-Batch-Inference-Role\"** but this can be changed by editing the *bedrock_batch_inference_role_name* variable. This role can be created using the instructions [here](https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference-permissions.html). If you are running this notebook as part of an AWS workshop event, this role may have been pre-created for you. If you are uanble to create this role, you can still continue through this workshop using the sample output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902aed04-49c7-487d-8bb0-3b757d85b4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_batch_inference_role_name = \"Bedrock-Batch-Inference-Role\"\n",
    "bedrock_batch_inference_role_arn = f\"arn:aws:iam::{ACCOUNT_ID}:role/{bedrock_batch_inference_role_name}\"\n",
    "\n",
    "def check_role_exists(role_name):\n",
    "    iam = boto3.client('iam')\n",
    "    try:\n",
    "        iam.get_role(RoleName=role_name)\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'NoSuchEntity':\n",
    "            return False\n",
    "        raise\n",
    "\n",
    "if check_role_exists(bedrock_batch_inference_role_name):\n",
    "    print(f\"Role '{bedrock_batch_inference_role_name}' exists\")\n",
    "else:\n",
    "    print(f\"Role '{bedrock_batch_inference_role_name}' does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a861a1-b97d-4f4d-82eb-a7b016aff1c6",
   "metadata": {},
   "source": [
    "### 2.11 Executing Bedrock Batch Inference\n",
    "\n",
    "Bedrock Batch Inference can take some time to finish execution. This section may take 30-60+ minutes to complete as Amazon Bedrock processes the Batch Inference Job. To skip this live batch processing, by default we will use sample output data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ea288c-c836-4dec-8d20-99da84f222fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SAMPLE_BATCH_INFERENCE_OUTPUT = True # Set to \"False\" to use live batch inference output.\n",
    "if USE_SAMPLE_BATCH_INFERENCE_OUTPUT:\n",
    "    print(\"We will use the stored sample_batch_inference_output.jsonl to demonstrate patient analysis.\")\n",
    "else:\n",
    "    print(\"We will send the input file to Bedrock Batch Inference and wait for the results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2001e3b6-7fc2-40d5-b19a-90f23cd45934",
   "metadata": {},
   "source": [
    "#### 2.11.1 Start Bedrock Batch Inference Job\n",
    "\n",
    "Bedrock Batch Inference jobs are asynchronous, so we immediately receive the job ARN (Amazon Resource Name) after submitting the job. This step is skipped if we are using the sample batch inference output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4c1911-6806-433e-b156-2a4510321236",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_SAMPLE_BATCH_INFERENCE_OUTPUT:\n",
    "    # Input data configuration\n",
    "    input_data_config = {\n",
    "      \"s3InputDataConfig\": {\n",
    "        \"s3Uri\": f\"s3://{SAGEMAKER_S3_BUCKET}/{bedrock_batch_processing_s3_input_filepath}\"\n",
    "      }\n",
    "    }\n",
    "    print(f\"Input data config: {input_data_config}\")\n",
    "    \n",
    "    bedrock_batch_processing_output_folder = \"Clinical_Trials_Gen_AI_Workshop_Batch_Inference_Ouput\"\n",
    "    bedrock_batch_processing_s3_output_uri = f\"s3://{SAGEMAKER_S3_BUCKET}/{bedrock_batch_processing_output_folder}/\"\n",
    "    # Output data configuration\n",
    "    output_data_config = {\n",
    "      \"s3OutputDataConfig\": {\n",
    "        \"s3Uri\": bedrock_batch_processing_s3_output_uri\n",
    "      }\n",
    "    }\n",
    "    print(f\"Output data config: {output_data_config}\")\n",
    "\n",
    "\n",
    "    # Create batch inference job\n",
    "    response = BEDROCK_CLIENT.create_model_invocation_job(\n",
    "      roleArn=bedrock_batch_inference_role_arn,\n",
    "      modelId=CLAUDE_3_SONNET_MODEL_ID,\n",
    "      inputDataConfig=input_data_config,\n",
    "      outputDataConfig=output_data_config,\n",
    "      jobName=\"BatchInferenceJob\"+generate_random_record_id()\n",
    "    )\n",
    "    \n",
    "    job_arn = response[\"jobArn\"]\n",
    "    print(f\"Job ARN: {job_arn}\")\n",
    "else:\n",
    "    print(\"Using sample batch inference output.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8f8635-d926-4ae0-9880-f301f25e44b5",
   "metadata": {},
   "source": [
    "#### 2.11.2 Query Bedrock Batch Inference Job Status\n",
    "\n",
    "We query the status of our Bedrock Batch Inference Job and print the status every 5 seconds until the job is completed. This step is skipped if we are using the sample batch inference output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08bf15c-2a5a-419f-97c4-9f4b68ea83a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not USE_SAMPLE_BATCH_INFERENCE_OUTPUT:\n",
    "    batch_inference_output_filename = \"batch_inference_output.jsonl\"\n",
    "    full_status = BEDROCK_CLIENT.get_model_invocation_job(jobIdentifier=job_arn)\n",
    "    status = full_status[\"status\"]\n",
    "    if VERBOSE_LOGGING:\n",
    "        print(f\"Full status: {full_status}\")\n",
    "    else:\n",
    "        print(f'Status: {full_status[\"status\"]}')\n",
    "    counter = 0\n",
    "    while status != \"Completed\":\n",
    "        print(f\"seconds: {counter}\")\n",
    "    \n",
    "        full_status = BEDROCK_CLIENT.get_model_invocation_job(jobIdentifier=job_arn)\n",
    "        status = full_status[\"status\"]\n",
    "        time.sleep(5)\n",
    "        counter += 5\n",
    "        if VERBOSE_LOGGING:\n",
    "            print(f\"Full status: {full_status}\")\n",
    "        else:\n",
    "            print(f'Status: {full_status[\"status\"]}')\n",
    "    \n",
    "    job_id = job_arn[job_arn.rfind(\"/\") + 1:]\n",
    "    print(f\"job_id: {job_id}\")\n",
    "    s3_output_file_location = f\"{bedrock_batch_processing_output_folder}/{job_id}/{batch_inference_input_filename}.out\"\n",
    "    #print(f\"s3_output_file_location: {s3_output_file_location}\")\n",
    "    S3_CLIENT.download_file(SAGEMAKER_S3_BUCKET, s3_output_file_location, batch_inference_output_filename)\n",
    "    print(f\"Downloaded batch inference output from s3://{SAGEMAKER_S3_BUCKET}/{s3_output_file_location} to {batch_inference_output_filename}\")\n",
    "else:\n",
    "    batch_inference_output_filename = f\"{part_3_resource_folder}/sample_batch_inference_output.jsonl\"\n",
    "    print(f\"Using sample batch inference output at {batch_inference_output_filename}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8179906e-ba5e-47ca-8f87-1c01c2e30ef7",
   "metadata": {},
   "source": [
    "### 2.12 Helper Function for flatening result output\n",
    "\n",
    "We will now parse the response of the LLM assistant which is currently a string of a multi-level JSON object. To streamline the analysis process, we will flatten this multi-level object to a single level by combining each level of the JSON object using an underscore (`_`) character.\n",
    "\n",
    "An example of this is transforming \n",
    "\n",
    "```\n",
    "\"Exclusion Criteria\": {\n",
    "        \"Known adverse reaction or tolerance to study medication\": {\n",
    "            \"Answer\": \"good candidate\",\n",
    "            \"Explanation\": \"There is no information provided about any known adverse reactions or tolerances to study medications.\"\n",
    "        },\n",
    "        \"Headache due to trauma\": {\n",
    "            \"Answer\": \"good candidate\",\n",
    "            \"Explanation\": \"There is no evidence in the patient record of a headache due to trauma.\"\n",
    "        }\n",
    "}\n",
    "```\n",
    "\n",
    "to\n",
    "\n",
    "```\n",
    "{\n",
    "    Exclusion Criteria_Known adverse reaction or tolerance to study medication_Answer : \"good candidate\",\n",
    "    Exclusion Criteria_Known adverse reaction or tolerance to study medication_Explanation : \"There is no information provided about any known adverse reactions or tolerances to study medications.\",\n",
    "    Exclusion Criteria_Headache due to trauma_Answer : \"good candidate\",\n",
    "    Exclusion Criteria_Headache due to trauma_Explanation : \"There is no evidence in the patient record of a headache due to trauma.\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e17c07d-91eb-479d-a65d-eec7aaab304b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_patient_result(model_completion):\n",
    "    print(model_completion)\n",
    "    raw_claude_output = model_completion.replace(\"\\n\",\"\")\n",
    "    xml_start_loc = raw_claude_output.find(\"<answer>\") + 8\n",
    "    xml_end_loc = raw_claude_output.find(\"</answer>\")\n",
    "    \n",
    "    if xml_start_loc == -1 or xml_end_loc == -1:\n",
    "        raise Exception(\"No <answer> xml tag detected in claude output\")\n",
    "    \n",
    "    def flat_keys(obj, new_obj={}, keys=[]):\n",
    "        for key, value in obj.items():\n",
    "            if isinstance(value, dict):\n",
    "                flat_keys(obj[key], new_obj, keys + [key])\n",
    "            else:\n",
    "                new_obj['_'.join(keys + [key])] = value\n",
    "        return new_obj\n",
    "    \n",
    "    flattened_patient_result = flat_keys(json.JSONDecoder().decode(raw_claude_output[xml_start_loc:xml_end_loc]))\n",
    "    return flattened_patient_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcc3ea7-26ea-426d-895d-2ef45352b417",
   "metadata": {},
   "source": [
    "### 2.13 Generate Patient Suitability Score\n",
    "\n",
    "We can now use the information we have about each study criteria for a patient to generate a discrete patient suitability score. We calculate this by adding up the score that the patient receives for each study criteria using the formula below\n",
    "\n",
    "| LLM Response of Study Criteria    | Score |\n",
    "| -------- | ------- |\n",
    "| (good candidate) Yes this patient is a good candidate for the scientific study. | 3 |\n",
    "| (likely good candidate) It is likely that this patient is a good candidate for the scientific study but more information might be needed. | 2 |\n",
    "| (likely bad candidate) It is unlikely that this patient is a good candidate for the scientific study but more information might be needed. | 1 |\n",
    "| (bad candidate) No this patient is not a good candidate for the scientific study. | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0285c20-eae7-4c9e-9668-370231831206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patient_score(flattened_patient_obj):\n",
    "    total_possible_score = 0\n",
    "    patient_score = 0\n",
    "    for criteria, result in flattened_patient_obj.items():\n",
    "    \n",
    "        if criteria.endswith(\"_Answer\"):\n",
    "            total_possible_score += 3\n",
    "        \n",
    "            match result:\n",
    "                case \"good candidate\":\n",
    "                    patient_score += 3\n",
    "                case \"likely good candidate\":\n",
    "                    patient_score += 2\n",
    "                case \"likely bad candidate\":\n",
    "                    patient_score += 1\n",
    "                case \"bad candidate\":\n",
    "                    patient_score += 0\n",
    "                case _:\n",
    "                    raise Exception(f\"Unexpected result {result} in flattened_patient_obj\")\n",
    "    \n",
    "    patient_suitability_percentage = round((patient_score/total_possible_score) * 100, 2)\n",
    "    return patient_suitability_percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3ab390-a238-4335-8a06-f8062174ad0b",
   "metadata": {},
   "source": [
    "### 2.14 Consolidate Patient Profile\n",
    "\n",
    "For each patient processed, we get the link of the consolidated patient information, the patient score, and the answers and explanations for each study criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4c4afa-c788-423c-be86-f0dbb7802a4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(batch_inference_output_filename) as f:\n",
    "    batch_output_raw = [json.loads(line) for line in f]\n",
    "\n",
    "consolidated_patient_result_output = []\n",
    "discarded_entries = 0\n",
    "for batch_output_item in batch_output_raw:\n",
    "    if len(batch_output_item[\"modelOutput\"]['content'][0]['text']) < 5:\n",
    "        discarded_entries += 1\n",
    "        continue\n",
    "        \n",
    "    patient_dict = flatten_patient_result(batch_output_item[\"modelOutput\"]['content'][0]['text'])\n",
    "    patient_dict[\"Score\"] = get_patient_score(patient_dict)\n",
    "    # if USE_SAMPLE_BATCH_INFERENCE_OUTPUT:\n",
    "    patient_dict[\"patient_id\"] = batch_output_item[\"recordId\"]\n",
    "    # else:\n",
    "    #     patient_dict[\"patient_id\"] = record_id_to_patient_id_map[batch_output_item[\"recordId\"]]\n",
    "\n",
    "    if VERBOSE_LOGGING:\n",
    "        print(patient_id)\n",
    "        print(patient_score)\n",
    "    consolidated_patient_result_output.append(patient_dict)\n",
    "\n",
    "print(f\"Discarded {discarded_entries} entries due to empty LLM output.\")\n",
    "print(f\"Finished processing {len(consolidated_patient_result_output)} patients.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50ffdb1-7a48-4111-b6d0-b424ec435016",
   "metadata": {},
   "source": [
    "### 2.15 Helper function for Color Coding answers\n",
    "\n",
    "We define a helper function to change the background color of a cell based on the answer from the LLM on clinical trial eligibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4384680d-d255-4836-b778-be4917794200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a color mapping\n",
    "color_mapping = {\n",
    "    'good candidate': '#4CAF50',  # Green\n",
    "    'likely good candidate': '#FFEB3B',  # Yellow\n",
    "    'likely bad candidate': '#FF6D00',  # Orange\n",
    "    'bad candidate': '#D50000',  # Red\n",
    "}\n",
    "\n",
    "# Function to generate cell style\n",
    "def get_cell_style(val):\n",
    "    color = color_mapping.get(str(val), \"#FFFFFF\")\n",
    "    return f'background-color: {color}; font-weight: bold;' if color != \"#FFFFFF\" else ''\n",
    "\n",
    "\n",
    "# Function to safely render cell content\n",
    "def render_cell(val):\n",
    "    if isinstance(val, str) and val.strip().startswith('<') and val.strip().endswith('>'):\n",
    "        return val  # Render as HTML\n",
    "    return html.escape(str(val))  # Escape as plain text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699fc1e1-bd22-4a13-ace9-0815b4f212fb",
   "metadata": {},
   "source": [
    "### 2.16 Display consolidated Patient Findings\n",
    "\n",
    "We display all of the patients and data we have processed in a consolidated webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf5b11d-6f81-440f-af1e-2ef5992cb529",
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_patient_df = pd.DataFrame(consolidated_patient_result_output)\n",
    "score_column = consolidated_patient_df.pop(\"Score\")\n",
    "patient_id_column = consolidated_patient_df.pop(\"patient_id\")\n",
    "consolidated_patient_df.insert(0, \"Score\", score_column)\n",
    "consolidated_patient_df.insert(0, \"patient_id\", patient_id_column)\n",
    "consolidated_patient_df = consolidated_patient_df.sort_values(by=\"Score\", ascending=False)\n",
    "# display(consolidated_patient_df)\n",
    "\n",
    "# Generate HTML\n",
    "html_template = '''\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Patient Information</title>\n",
    "    <style>\n",
    "        body, html {{\n",
    "            height: 100%;\n",
    "            margin: 0;\n",
    "            padding: 0;\n",
    "            font-family: Arial, sans-serif;\n",
    "        }}\n",
    "        .container {{\n",
    "            display: flex;\n",
    "            height: 100vh;\n",
    "        }}\n",
    "        .table-container {{\n",
    "            flex: 1;\n",
    "            overflow: auto;\n",
    "        }}\n",
    "        .side-panel {{\n",
    "            width: 0;\n",
    "            background-color: #f1f1f1;\n",
    "            position: fixed;\n",
    "            top: 0;\n",
    "            right: 0;\n",
    "            height: 100%;\n",
    "            z-index: 1000;\n",
    "            box-shadow: -2px 0 5px rgba(0,0,0,0.2);\n",
    "            overflow: hidden;\n",
    "            transition: width 0.3s;\n",
    "            display: flex;\n",
    "            flex-direction: column;\n",
    "        }}\n",
    "        .resize-handle {{\n",
    "            width: 10px;\n",
    "            background-color: #ccc;\n",
    "            cursor: ew-resize;\n",
    "            position: absolute;\n",
    "            top: 0;\n",
    "            bottom: 0;\n",
    "            left: 0;\n",
    "        }}\n",
    "        .side-panel-header {{\n",
    "            position: sticky;\n",
    "            top: 0;\n",
    "            background-color: #f1f1f1;\n",
    "            padding: 20px;\n",
    "            z-index: 2;\n",
    "            border-bottom: 1px solid #ddd;\n",
    "        }}\n",
    "        .side-panel-content {{\n",
    "            flex-grow: 1;\n",
    "            overflow-y: auto;\n",
    "            overflow-x: hidden;\n",
    "            padding: 0 20px 20px 20px;\n",
    "        }}\n",
    "        .close-btn {{\n",
    "            position: sticky;\n",
    "            top: 10px;\n",
    "            float: right;\n",
    "            font-size: 36px;\n",
    "            cursor: pointer;\n",
    "        }}\n",
    "        table {{\n",
    "            border-collapse: separate;\n",
    "            border-spacing: 0;\n",
    "            width: 100%;\n",
    "        }}\n",
    "        th, td {{\n",
    "            border: 1px solid #ddd;\n",
    "            padding: 8px;\n",
    "            text-align: center;\n",
    "        }}\n",
    "        th {{\n",
    "            background-color: #f2f2f2;\n",
    "            position: sticky;\n",
    "            top: 0;\n",
    "            z-index: 10;\n",
    "        }}\n",
    "        .sticky-col {{\n",
    "            position: sticky;\n",
    "            background-color: #f2f2f2;\n",
    "            z-index: 5;\n",
    "        }}\n",
    "        .sticky-col.header {{\n",
    "            z-index: 11;\n",
    "        }}\n",
    "        th:first-child, td:first-child {{\n",
    "            left: 0;\n",
    "        }}\n",
    "        th:nth-child(2), td:nth-child(2) {{\n",
    "            left: 40px;\n",
    "        }}\n",
    "        .patient-id {{\n",
    "            cursor: pointer;\n",
    "            color: blue;\n",
    "            text-decoration: underline;\n",
    "        }}\n",
    "        .patient-info-container {{\n",
    "            overflow-x: auto;\n",
    "            white-space: nowrap;\n",
    "        }}\n",
    "        #patientInfo {{\n",
    "            display: inline-block;\n",
    "            white-space: pre;\n",
    "            font-family: monospace;\n",
    "        }}\n",
    "        .horizontal-scroll {{\n",
    "            overflow-x: scroll;\n",
    "            overflow-y: hidden;\n",
    "            height: 12px;\n",
    "            background-color: #ddd;\n",
    "            position: sticky;\n",
    "            top: 0;\n",
    "            z-index: 3;\n",
    "            opacity: 1;\n",
    "            transition: opacity 0.3s;\n",
    "        }}\n",
    "        .horizontal-scroll::-webkit-scrollbar {{\n",
    "            height: 12px;\n",
    "        }}\n",
    "        .horizontal-scroll::-webkit-scrollbar-thumb {{\n",
    "            background-color: #888;\n",
    "            border-radius: 6px;\n",
    "        }}\n",
    "        .horizontal-scroll::-webkit-scrollbar-track {{\n",
    "            background-color: #ddd;\n",
    "            height: 5px !important;\n",
    "        }}\n",
    "        .scroll-content {{\n",
    "            height: 1px;\n",
    "        }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <div class=\"table-container\">\n",
    "            <table>\n",
    "                <thead>\n",
    "                    <tr>\n",
    "                        <th class=\"sticky-col header\" style=\"min-width: 40px;\">#</th>\n",
    "                        <th class=\"sticky-col header\" style=\"min-width: 100px;\">patient_id</th>\n",
    "                        {table_headers}\n",
    "                    </tr>\n",
    "                </thead>\n",
    "                <tbody>\n",
    "                    {table_rows}\n",
    "                </tbody>\n",
    "            </table>\n",
    "        </div>\n",
    "        <div id=\"sidePanelContainer\" class=\"side-panel\">\n",
    "            <div id=\"resizeHandle\" class=\"resize-handle\"></div>\n",
    "            <div class=\"side-panel-header\">\n",
    "                <span class=\"close-btn\" onclick=\"closeNav()\">&times;</span>\n",
    "                <h2>Patient Information</h2>\n",
    "            </div>\n",
    "            <div class=\"horizontal-scroll\">\n",
    "                <div class=\"scroll-content\"></div>\n",
    "            </div>\n",
    "            <div class=\"side-panel-content\">\n",
    "                <div class=\"patient-info-container\">\n",
    "                    <pre id=\"patientInfo\"></pre>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    <script>\n",
    "        const patientInfoMap = {patient_info_json};\n",
    "        const sidePanel = document.getElementById(\"sidePanelContainer\");\n",
    "        const resizeHandle = document.getElementById('resizeHandle');\n",
    "        const horizontalScroll = document.querySelector('.horizontal-scroll');\n",
    "        const patientInfoContainer = document.querySelector('.patient-info-container');\n",
    "        const scrollContent = document.querySelector('.scroll-content');\n",
    "        const patientInfoElement = document.getElementById(\"patientInfo\");\n",
    "        \n",
    "        let isResizing = false;\n",
    "        let startX;\n",
    "        let startWidth;\n",
    "\n",
    "        function showPatientInfo(patientId) {{\n",
    "            if (patientInfoMap[patientId]) {{\n",
    "                let patientData = JSON.parse(patientInfoMap[patientId]);\n",
    "                let formattedInfo = JSON.stringify(patientData, null, 4);\n",
    "                patientInfoElement.textContent = formattedInfo;\n",
    "                sidePanel.style.width = \"400px\";\n",
    "                updateScrollContent();\n",
    "            }} else {{\n",
    "                patientInfoElement.textContent = \"No information available for this patient.\";\n",
    "                sidePanel.style.width = \"400px\";\n",
    "            }}\n",
    "        }}\n",
    "\n",
    "        function closeNav() {{\n",
    "            sidePanel.style.width = \"0\";\n",
    "        }}\n",
    "\n",
    "        function updateScrollContent() {{\n",
    "            requestAnimationFrame(() => {{\n",
    "                scrollContent.style.width = `${{patientInfoElement.scrollWidth}}px`;\n",
    "            }});\n",
    "        }}\n",
    "\n",
    "        resizeHandle.addEventListener('mousedown', (e) => {{\n",
    "            isResizing = true;\n",
    "            startX = e.clientX;\n",
    "            startWidth = parseInt(getComputedStyle(sidePanel).width, 10);\n",
    "            document.addEventListener('mousemove', resize);\n",
    "            document.addEventListener('mouseup', stopResize);\n",
    "        }});\n",
    "\n",
    "        function resize(e) {{\n",
    "            if (!isResizing) return;\n",
    "            \n",
    "            const width = startWidth + (startX - e.clientX);\n",
    "            \n",
    "            if (width > 200 && width < window.innerWidth - 100) {{\n",
    "                sidePanel.style.width = `${{width}}px`;\n",
    "                updateScrollContent();\n",
    "            }}\n",
    "        }}\n",
    "\n",
    "        function stopResize() {{\n",
    "            isResizing = false;\n",
    "            document.removeEventListener('mousemove', resize);\n",
    "            document.removeEventListener('mouseup', stopResize);\n",
    "        }}\n",
    "\n",
    "        horizontalScroll.addEventListener('scroll', () => {{\n",
    "            patientInfoContainer.scrollLeft = horizontalScroll.scrollLeft;\n",
    "        }});\n",
    "\n",
    "        patientInfoContainer.addEventListener('scroll', () => {{\n",
    "            horizontalScroll.scrollLeft = patientInfoContainer.scrollLeft;\n",
    "        }});\n",
    "\n",
    "        // Refresh scroll content width on window resize\n",
    "        window.addEventListener('resize', updateScrollContent);\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "# Generate table headers\n",
    "table_headers = ' '.join(f'<th>{html.escape(str(col))}</th>' for col in consolidated_patient_df.columns[1:])\n",
    "\n",
    "# Generate table rows\n",
    "table_rows = ''\n",
    "for index, row in consolidated_patient_df.iterrows():\n",
    "    table_rows += f'<tr>'\n",
    "    table_rows += f'<td class=\"sticky-col\">{index}</td>'\n",
    "    table_rows += f'<td class=\"sticky-col patient-id\" onclick=\"showPatientInfo(\\'{row[\"patient_id\"]}\\')\">{row[\"patient_id\"]}</td>'\n",
    "    table_rows += ' '.join(f'<td style=\"{get_cell_style(val)}\">{render_cell(val)}</td>' for val in row[1:])\n",
    "    table_rows += '</tr>'\n",
    "    \n",
    "# Load or create patient_info_json\n",
    "if not USE_SAMPLE_BATCH_INFERENCE_OUTPUT:\n",
    "    patient_info_json = json.dumps(record_id_to_patient_info_map)\n",
    "else:\n",
    "    sample_record_id_to_patient_info_map_filename = f\"{part_3_resource_folder}/sample_record_id_to_patient_info_map.json\"\n",
    "    with open(sample_record_id_to_patient_info_map_filename, 'r') as file:\n",
    "        patient_info_json = file.read()\n",
    "\n",
    "# Fill in the template\n",
    "html_content = html_template.format(\n",
    "    table_headers=table_headers,\n",
    "    table_rows=table_rows,\n",
    "    patient_info_json=patient_info_json\n",
    ")\n",
    "\n",
    "# Save the HTML content to a file\n",
    "consolidated_patient_table_name = \"consolidated_patient_table.html\"\n",
    "with open(consolidated_patient_table_name, 'w') as f:\n",
    "    f.write(html_content)\n",
    "\n",
    "print(f\"Finished generating {consolidated_patient_table_name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b12b2a-fc69-4e1d-a70d-c1f3e4595fe4",
   "metadata": {},
   "source": [
    "### 2.17 View Consolidated Patient Findings\n",
    "\n",
    "Now let's walk through the consolidated patient table together. To open the Consolidated Patient Table HTML File, you may either \n",
    "\n",
    "    1. Right-click the HTML file in the sidebar and click download. Open this file from your computer's download folder in any browser of your choice.\n",
    "    2. Double-click the HTML file in the sidebar to open it in a new tab in JupyterLab. Click \"Trust HTML\" in the top left corner if this is your first time opening the file.\n",
    "\n",
    "You'll notice our sample study and output has resulted in a wide variety of responses across patients and clinical study criteria. We have sorted the table based on our calculated scoring with every criteria equally weighted. This view will now allow you to investigate each patient and criteria in depth to determine the best patient(s) to select for a clinical trial. Clicking the blue hyperlinked patient_id shows the details that we fed as input to our LLM for that patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037f6c86-4b3a-496e-a7a6-289889bf111d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
